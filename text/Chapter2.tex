\section{Обзор существующих решений}
\label{sec:Chapter2} \index{Chapter2}

Исследователи уже более десяти лет ведут работы в направлении автоматизации тестирования мобильных приложений на основе взаимодействия с графическим интерфейсом пользователя. В этой части будут описаны существующие подходы к решению похожих задач и выделены их преимущества и недостатки.

Для начала стоит отметить, что все существующие подходы можно разбить на три группы:

\begin{itemize}

\item подходы, основанные на случайных нажатиях
 
\item подходы, основанные на моделях взаимодействия
 
\item подходы, основанные на алгоритмах оптимального поиска

\end{itemize}

Подходы, основанные на случайных нажатиях -- одни из самых популярных методов обнаружения сбоев в приложении. Идея такого тестирования проста: с большой частотой генерируются нажатия на экран в случайной позиции. Этот метод может быть пригодным для эффективного стресс-тестирования (тестирование за пределами нормального использования приложения). Например, Android Monkey~\cite{studio2017ui} -- инструмент тестирования по стратегии <<черный ящик>> поставляемый вместе с Android SDK (Software Development Kit). Благодаря своей простоте этот инструмент приобрел значительную популярность в сообществе разработчиков. Помимо простоты, он продемонстрировал хорошую совместимость с множеством Android платформ, что сделало его промышленным стандартом~\cite{wang2018empirical}. Однако Android Monkey требуется много времени для генерации сложной последовательности событий, способной посещать необычные состояния. В последовательности взаимодействий, генерируемой этой стратегией, часто встречаются повторяющиеся действия, которые не приводят к исследованию новых состояний. Также недостатком этого подхода являются многочисленные нажатия на неинтерактивные элементы, что вовсе не может изменить текущего состояния приложения. В связи с этим был разработан более совершенный подход, призванный исправить недостатки Monkey -- DynoDroid~\cite{machiry2013dynodroid}. В нем используются дополнительные эвристики, благодаря которым случайный выбор действия зависит от состояния в котором находится тестируемое приложение. Одна из основных модификаций этого инструмента -- возможность использовать заранее прописанный пользователем сценарий при попадании в определенные состояния. Это дает значительный прирост производительности тестирования, но требует вмешательства пользователя в процесс формирования тестовых нажатий. Общий недостаток случайных стратегий в том, что сбои и ошибки, найденные во время тестирования, трудно воспроизводимы. Что еще более важно, такой подход может генерировать тысячи нажатий в секунду, что нехарактерно для стандартных сценариев использования смартфона.

Тестирование на основе моделей предполагает использование некоторой модели для представления взаимодействия пользователя с графическим интерфейсом приложения. Модель разрабатывается вручную или автоматически путем исследования кода или непосредственного взаимодействия с приложением. Тестирование на основе модели может производиться с использованием разных стратегий: обход в глубину, обход в ширину, гибридная или стохастическая. Важным является то, что этот подход чувствителен к точности построенной модели. В частности, слишком чувствительная модель, построенная с учетом малейших изменений графического интерфейса не сможет эффективно исследовать новые состояния, так как любое нажатие будет приводить к их смене. Наоборот, если модель приложения описывается слишком обобщенными состояниями это может привести к потере эффективности тестирования, так как смена состояния может быть пропущена. Метод $A^3E$~\cite{azim2013targeted} исследует приложения в два этапа: статический анализ кода приложения для построения высокоуровневой модели взаимодействия, и целевое исследование, которое на основе модели приложения позволяет быстро посещать состояния, сложно достижимые при нормальном использовании. Однако, $A^3E$ для представления состояния использует Активность, без учета того, что одна Активность может содержать множество мелких состояний. Это приводит к ухудшению производительности тестирования. Еще один современный подход, основанный на модели взаимодействия - Humanoid~\cite{li2019deep}. Этот подход использует глубокие нейронные сети для предсказания нажатий в текущем состоянии. Основная идея этого подхода в том, чтобы обучить модель взаимодействовать с приложением так, как это делал бы человек. На основе набора данных пользовательских взаимодействий с приложением, глубокая нейронная сеть пытается выучить основные шаблоны взаимодействий и использовать их для тестирования. Этот подход превосходит многие аналогичные инструменты. Именно с этим подходом будут сравниваться методы обучения с подкреплением.

Основной представитель третьей группы методов -- Sapienz~\cite{mao2016sapienz}. Этот подход основан на многоцелевом тестировании. Sapienz пытается добиться  сразу нескольких целей: оптимизации длины тестовых последовательностей, максимальный охват состояний приложения и максимальное количество найденных ошибок. С помощью генетических алгоритмов удается найти оптимальную стратегию тестирования, способную выполнять сразу все поставленные цели.

Современные работы в этой области ведутся над моделе-ориентированными методами, так как они показывают передовые результаты и к ним можно применять алгоритмы машинного обучения. Уже есть работы, которые пытаются внедрить алгоритмы машинного обучения в тестирования мобильных приложений через графический интерфейс~\cite{li2019deep, harries2020drift}. Поставленная задача хорошо подходит под группу алгоритмов машинного обучения -- обучение с подкреплением. Есть несколько работ, которые описывают применение подходов обучения с подкреплением для тестирования разного вида программ.

Первая попытка внедрения алгоритма обучения с подкреплением Q-learning для тестирования приложений была сделана в 2018 году~\cite{koroglu2018qbe}. Исследователи предложили подход под названием QBE. Этот подход, на основе заранее построенной модели, способен извлекать сложные последовательности действий, которые приводят к сбоям. Таким образом, перед тестированием необходимо собрать информацию о том, какие жесты пользователя приводят к ошибкам. Функция награды, которая является основным элементом стратегий обучения с подкреплением, награждает алгоритм за достижения состояний с ошибками и тем самым мотивирует чаще выполнять действия, приводящие к таким состояниям. Благодаря примитивному представлению состояния -- количество интерактивных элементов в текущий момент времени, этот подход может построить общую Q-таблицу для всех приложений. 

С 2018 года область обучения с подкреплением в задаче тестирования мобильных приложений стала активно развиваться. Появились работы, использующие Q-learning алгоритм, целью которых является обучение под единственное приложение~\cite{adamo2018reinforcement, vuong2018reinforcement}. Идея таких подходов в том, чтобы объединить этапы обучения и тестирования, то есть формирование Q-таблицы для приложения происходит одновременно с тестированием приложения. Опять же, основной элемент, устанавливающий цель тестирования -- функция награды. Так в~\cite{adamo2018reinforcement} используется функция награды в виде числа, обратно пропорционального количеству нажатий на данный интерактивный элемент в данном состоянии. Это позволяет производить тестирование без повторений одних и тех же действия до тех пор, пока не будут выполнены другие действия в этом состоянии. В~\cite{vuong2018reinforcement} используют похожий подход, однако функция награды модифицируется. Помимо обратной частоты нажатий к функции добавляется слагаемое, описывающее степень изменения нового состояния: отношение новых интерактивных элементов ко всем интерактивным элементам.

Еще одна современная работа использующая обучение с подкреплением -- DRIFT~\cite{harries2020drift}. В DRIFT используется идея моделирования Q-таблицы на основе графовой нейронной сети. Хотя цель этой работы -- тестирование программ под операционной системой Windows, идея моделирования Q-таблицы, кажется перспективной и будет использоваться в этой работе. 

Несмотря на то, что уже существуют подходы, использующие алгоритмы обучения с подкреплением для тестирования приложений, все они либо примитивны, либо не подходят под ограничения поставленной задачи. Так, в работах~\cite{koroglu2018qbe, adamo2018reinforcement} рассматриваются слишком простые эвристики. В~\cite{harries2020drift} рассматривается тестирование приложений для другой операционной системы. 