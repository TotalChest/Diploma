\section{Исследование и построение решения задачи}
\label{sec:Chapter3} \index{Chapter3}

Перед тем как рассказывать о подходах к решению поставленной задачи нужно дать формальное описание того, как работают алгоритмы обучения с подкреплением.

\subsection{Общие сведения об обучении с подкреплением}

Обучение с подкреплением~\cite{szepesvari2010algorithms} -- это одна их больших ветвей машинного обучения. В отличие от других ветвей, таких как обучение с учителем или без учителя, обучение здесь происходит путем учета вознаграждений и наказаний за взаимодействие с окружающей средой. Идея таких подходов основана на концепции поведенческой психологии: методом проб и ошибок достичь максимального результата. В методах обучения с подкреплением основными компонентами являются среда и агент. Агент выступает в качестве объекта, который выполняет действия в среде для достижения определенной цели. В ходе выполнения последовательности действий агент получает информацию о среде. Среда это пространство, в котором агент учится добиваться поставленных целей. 

Помимо агента и среды в обучении с подкреплением есть еще три сущности: функция вознаграждения, действия и состояние. Состояние описывает текущую ситуацию в окружающей среде. В каждом состоянии у агента есть определенное количество возможных действий. За каждое действие предусмотрена награда, которая оценивает важность того или иного действия в текущем состоянии. Задача агента в том, чтобы в каждом состоянии научиться выбирать такое действие, которое приводит его к максимальной суммарной награде. Попробовав все действия в текущем состоянии, агент научится определять какое из них с большей вероятностью приведет его к поставленной цели.

\begin{figure}[h]
\center{\includegraphics[width=250px]{RLexplanation.png}}
\caption{Один раунд обучения с подкреплением.}
\label{RLexplanation}
\end{figure}

Во всех существующих подходах к тестированию программного обеспечения через взаимодействие с графическим интерфейсом, использующих подход обучения с подкреплением, применяется техника Q-learning. Q-learning -- это самый распространенный и хорошо изученный подход обучения с подкреплением. В нашем случае эта техника используется для поиска оптимального действия для данного состояния мобильного приложения. Различные стратегии и функции наград устанавливают правила, по которым агент должен отдавать предпочтение определенному действию. После выбора и воспроизведения действия среда изменяет свое состояние на новое, и агент получает награду за выполненное действие. На Рис.~\ref{RLexplanation} показана классическая схема одного раунда взаимодействия агента и среды с помощью техники Q-learning:

\begin{enumerate} 

\item Агент получает от окружения состояние $s_t$.

\item Агент выбирает и воспроизводит действие $a_t \in A(s_t)$, где $A(s_t)$ -- множество доступных действий в состоянии $s_t$.

\item Окружение рассчитывает значение функции награды $R(s_t, a_t)$ исходя из состояния $s_t$ и выбранного действия в этом состоянии $a_t$.

\item Окружение рассчитывает новое состояние $s_{t+1}$ исходя из состояния $s_t$ и выбранного действия в этом состоянии $a_t$.

\item Агент получает значение награды $r_t$ и переходит в состояние $s_{t+1}$.

\end{enumerate}

Таким образом, цель агента полностью описывается с помощью функции награды. Поэтому в задачах обучения с подкреплением важно подобрать такую функцию награды, которая будет максимально соответствовать поставленной цели.

За выбор определенного действия в текущем состоянии отвечают так называемые Q-значения. Каждой паре $(s_t, a_t)$ соответствует свое Q-значение, которое описывает потенциальную суммарную награду, которую может получить агент, выбрав действие $a_t$ в состоянии $s_t$. Совокупность всех таких пар описывает Q-таблицу. Таким образом, выбор действия агентом, сводится к тому, чтобы найти максимальное Q-значение в текущем состоянии.

Обучение же модели состоит в том, чтобы подобрать такие Q-значения, которые будут близко описывать потенциальную суммарную награду за выполнение определенного действия. Уравнение, которым описывается обновление Q-значений после получения награды выглядит следующим образом

\begin{equation}
\label{eq_qlearning} 
{\displaystyle
Q(s_t, a_t) = R(s_t, a_t) + \gamma \max_{a_{t+1}}{Q(s_{t+1}, a_{t+1})} ,
}
\end{equation}
где $R(s_t, a_t)$ -- функция награды, $\gamma$ -- фактор дисконтирования, описывающий баланс между немедленной наградой и наградой за будущие действия. Таким образом, за большое число итераций Q-таблица стабилизируется и будет отражать реальную награду за каждое действие.


\subsection{Информация об окружении для тестирования}

Перед тем, как рассматривать конкретные алгоритмы и их реализации, нужно понимать как происходит взаимодействие с устройством. В качестве устройства во всех экспериментах будет выступать официальный эмулятор операционной системы Android. Удобным инструментом для программного взаимодействия с телефоном является утилита командной строки Android Debug Bridge (adb). Именно ее использует DroidBot~\cite{li2017droidbot} для взаимодействия с подключенным устройством. Все реализованные в работе алгоритмы работают на основе генератора тестов DroidBot. Он удобен тем, что может взаимодействовать с приложениями без модификации системы и самого приложения. Это делает DroidBot совместимым со всеми устройствами и приложениями. Также он предоставляет возможность подключать собственные стратегии тестирования, что актуально для этой работы. Еще одна причина, по которой был выбран этот инструмент -- подробный отчет о тестировании, из которого можно получить нужные метрики.

\begin{figure}[h]
\center{\includegraphics[width=460px]{InteractionScheme.png}}
\caption{Цикл взаимодействия устройства, тестового генератора DroidBot и алгоритма генерации нажатий.}
\label{InteractionScheme}
\end{figure}

Для лучшего понимания процесса тестирования, весь цикл взаимодействия тестового генератора DroidBot и алгоритма генерации нажатий с устройством представлен на Рис.~\ref{InteractionScheme}. Тестовый генератор способен получать текущее состояние приложения от устройства и воспроизводить нажатия на экране смартфона. Получив состояние от устройства, тестовый генератор обрабатывает его и в удобном виде передает алгоритму генерации нажатий.  Алгоритм генерации нажатий путем взаимодействия с тестовым генератором получает обработанное состояние приложения и генерирует жест для воспроизведения на устройстве. Этот жест воспроизводится путем нажатия в нужную координату экрана, и цикл взаимодействия повторяется.

Как было указано в постановке задачи, в работе используется две метрики из четырех основных, введенных в главе~\ref{sec:Chapter0}. Их числовые значения наиболее информативны и понятны. Они обе сопряжены с понятием состояния приложения. Проблема представления состояния приложения в данной работе решается следующим образом: будем считать, что состояния являются одинаковыми, если список возможных взаимодействий в этих состояниях совпадает. То есть состоянием в этой работе является список возможных взаимодействий в текущий момент времени. Действие в данном состоянии можно описать парой (тип действия, интерактивный элемент). Каждый интерактивный элемент имеет свои типы взаимодействия с ним. Эту информацию предоставляет тестовый генератор DroidBot. Таким образом, в каждый момент времени у нас есть доступ к текущему состоянию и всевозможным взаимодействиям в этом состоянии.

Что касается Активности приложения, то ее тоже можно сопоставлять с некоторым понятием состояния. Отличие состоит в том, что Активность приложения является более обобщенным понятием состояния, которое является единым для всех приложений и описывается классом языка программирования. Несколько состояний в нашем понимании могут находиться в одной Активности. Измерения только Активности приложения может быть недостаточно, так как приложение может содержать только одну Активность и при этом множество состояний. Таким образом, метрика Активности приложения всегда будет ниже метрики состояний, но при этом эти метрики независимы и хорошо отражают степень покрытия приложения при тестировании. Если говорить про две другие метрики, то они не будут рассматриваться в этой работе. Причина отказа от метрики с покрытием кода состоит в том, что тестирование в данной работе проводится по стратегии <<черный ящик>>: без доступа к исходному коду. Отказ от метрики с количеством найденных ошибок связан с тем, что эксперименты для проверки эффективности тестирования проводятся на уже готовых приложениях, количество сбоев в которых крайне мало.

Чем больше типов взаимодействия с экраном смартфона будет поддержано во время тестирования, тем потенциально большего покрытия удастся достичь. Поэтому будем использовать все типы жестов, которые предоставляет интерфейс DroidBot: нажатие, двойное нажатие, удержание, вставка текста, горизонтальная и вертикальная прокрутка, кнопки <<Меню>> и <<Назад>>.

Как и требовалось в постановке, все эксперименты в работе будут проводиться на одной и той же ЭВМ. В нашем случае спецификация ЭВМ следующая: \texttt{Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz, 64 GB RAM, GeForce GTX 1080}.

\subsection{Настройка гиперпараметров}

В этой части работы будет определено два основных параметра для всех алгоритмов: количество запусков и время тестирования. Для проведения всех экспериментов был реализован базовый алгоритм Q-learning~\cite{adamo2018reinforcement}, подробное описание которого будет дано позже.

В некоторых алгоритмах, которые будут рассматриваться в дальнейшем, присутствует элемент случайности. Таким образом, тестирование от запуска к запуску может показывать различные значения метрик. Однако это не единственный источник недетерминированности. Так как запуски проводятся на эмуляторе реального устройства, то его работа может сопровождаться некоторыми зависаниями. В добавок ко всему долгая загрузка страниц приложения, наличие анимации и задержки Android Debug Bridge могут привести к различным результатам одного и того же алгоритма. Степень недетерминированности базового алгоритма можно оценить по Рис.~\ref{indeterminacy}. На нем представлены результаты тестирования шести приложений в виде диаграмм размаха. Хорошо видна необходимость проводить эксперименты по несколько раз и результат усреднять. Для всех дальнейших алгоритмов измерения будут проводиться по пять раз. 

Постановка задачи требовала от алгоритма ограниченности по времени, так как в практических приложениях тестирование должно занимать разумное количество времени. Проведем эксперименты на базовом Q-learning алгоритме для вычисления оптимального времени тестирования. По результатам замеров на шести приложениях (Рис.~\ref{metric_per_time}) можно видеть, что графики зависимости метрик от времени выпуклы вверх. Это означает, что с течением времени эффективность поиска новых состояний уменьшается. Для некоторых приложений есть ощутимая разница между 10 и 15 минутами тестирования, но для большинства это несущественный прирост метрики. В качестве компромисса все дальнейшие эксперименты будут проводиться с ограничением времени 12 минут.

\begin{figure}[h]
\centering
\subfloat[Разброс метрики уникальных состояний.]{\includegraphics[width=240px]{states_indeterminacy.jpg}} 
\subfloat[Разброс метрики уникальных Активностей.]{\includegraphics[width=240px]{activities_indeterminacy.jpg}}
\caption{Недетерминированность тестирования.}
\label{indeterminacy}
\end{figure}

\begin{figure}[h]
\centering
\subfloat[Зависимость метрики состояний от времени.]{\includegraphics[width=240px]{states_per_time.jpg}} 
\subfloat[Зависимость метрики Активности от времени.]{\includegraphics[width=240px]{activities_per_time.jpg}}
\caption{Зависимость метрик от времени.}
\label{metric_per_time}
\end{figure}

\subsection{Приложения}

Чтобы сделать процесс тестирования наиболее честным, возьмем для тестирования приложения различных категорий и степеней сложности. Всего будет использоваться 10 современных приложений: \texttt{Booking}, \texttt{EBAY}, \texttt{The New York Times}, \texttt{Applebee's}, \texttt{FaceApp}, \texttt{The Wall Street Journal}, \texttt{AliExpress}, \texttt{Domino's Pizza}, \texttt{ColorNote}, \texttt{Wikipedia}. В их числе есть как простые приложения с небольшим количеством состояний, так и масштабные приложения с платформы Google Play.

\subsection{Независимые от приложения подходы}

Идеальный инструмент тестирования мобильных приложений -- это инструмент, который способен производить эффективное тестирование для произвольного приложения. Именно в поисках такого инструмента исследование начинается с алгоритмов обучения с подкреплением, которые обучают обобщенную модель, способную взаимодействовать с произвольным приложением. Такой подход связан с рядом упрощений. Например, введение обобщенного состояния и обобщенного действия, которые подходят под все приложения. Обучение таких моделей происходит онлайн (во время тестирования), так как только методом проб и ошибок можно научить Q-learning алгоритмы производить эффективные нажатия.

\subsubsection{Абстрактные состояния}

Первая модель, которая была реализована и внедрена в тестирующую систему DroidBot -- это модель основанная на алгоритме, описанном в~\cite{koroglu2018qbe}. Это классическая техника Q-learning, в которой состояние описывается количеством интерактивных элементов в текущий момент времени, а действие в текущем стоянии описывается лишь типом жеста. Таким образом, модель в зависимости от количества интерактивных элементов в текущем состоянии должна предсказывать тип взаимодействия, приводящий к потенциально новым состояниям.

В качестве функции награды была выбрана величина, равная количеству интерактивных элементов в состоянии, в которое перешло приложение после воспроизведения действия. Эта величина способствует посещению состояний с потенциально большой вариативностью для исследования. Таким образом, алгоритм учится выбирать те действия, которые приводят его к <<богатым>> состояниям. Результаты тестирования этого подхода приведены в Таблице~\ref{app_free_table}.

Представление состояния только количеством интерактивных элементов не может детально описать состояние приложения. В следующем пункте будет рассмотрено альтернативное представление состояния.

\subsubsection{Нейронная сеть}

Для улучшения предыдущего подхода, было решено представлять абстрактное состояние в виде изображения. А именно трехмерного тензора размером $(180, 320, 2)$. Можно заметить, что он представляет из себя две матрицы размера $(180, 320)$. Каждая из двух матриц состоит из нулей и единиц. Единицами на первой матрице выделены интерактивные элементы текущего экрана приложения, на второй -- текстовые элементы. Таким образом удалось ввести общее состояние для всех приложений. Для обработки изображений хорошо зарекомендовали себя сверточные нейронные сети. Именно их и будем использовать для предсказания Q-значений.

В этом подходе сверточная нейронная сеть будет моделировать Q-таблицу. В классическом подходе, Q-таблица должна по паре $(s_t, a_t)$ возвращать Q-значение. Для имитации такого возвращения нейронная сеть будет одновременно получать состояние и действие, а возвращать единственное значение -- Q-значение. Чтобы на вход сверточной нейронной сети передать одновременно состояние и действие, модифицируем вход: будем добавлять к тензору состояния еще один бинарный слой размера $(180, 320)$. Этот слой будет выделять единицами точку прикосновения пользователя. Архитектура нейронной сети (Рис.~\ref{neural_network}) была выбрана небольшая, так как данных для обучения будет немного. 

\begin{figure}[h]
\center{\includegraphics[width=400px]{NeuralNetwork.png}}
\caption{Архитектура нейронной сети для независимого от приложения подхода.}
\label{neural_network}
\end{figure}

Обучение такой сети производится на каждой итерации взаимодействия. Небольшие наборы данных, собранные на всех предыдущих итерациях, постепенно обновляют веса модели и формируют нужную стратегию. Функция награды точно такая же, как и в предыдущем пункте: способствующая более широкому исследованию.

Результаты тестирования этой стратегии приведены в Таблице~\ref{app_free_table}. Для большей наглядности результаты первых двух подходов представлены на Рис.~\ref{app_free_picture} в виде диаграмм размаха.

\begin{table}[h!]
    \centering
    \begin{tabular}{ | l | *{4}{>{\centering\arraybackslash}p{70} |}}
        \hline
        Приложения & \multicolumn{2}{c|}{Абстрактные состояния} & \multicolumn{2}{c|}{Нейронная сеть} \\
        \hline
        ALIEXPRESS &
        \textbf{25} &
        9 & 
        24 & 
        \textbf{12} \\
        \hline
        APPLEBEES &
        \textbf{30} &
        \textbf{7} & 
        23 & 
        \textbf{7} \\
        \hline
        BOOKING &
        \textbf{76} &
        \textbf{18} & 
        45 & 
        13 \\
        \hline
        COLORNOTE &
        52 &
        \textbf{6} & 
        \textbf{81} & 
        \textbf{6} \\
        \hline
        DOMINOS &
        16 &
        \textbf{1} & 
        \textbf{20} & 
        \textbf{1} \\
        \hline
        EBAY &
        \textbf{41} &
        \textbf{8} & 
        24 & 
        6 \\
        \hline
        FACEAPP &
        36 &
        \textbf{2} & 
        \textbf{37} & 
        \textbf{2} \\
        \hline
        NYTIMES &
        \textbf{34} &
        \textbf{6} & 
        27 & 
        \textbf{6} \\
        \hline
        WIKIPEDIA &
        \textbf{118} &
        \textbf{11} & 
        65 & 
        9 \\
        \hline
        WSJ &
        36 &
        7 & 
        \textbf{53} & 
        \textbf{8} \\
        \hline
    \end{tabular}
    \caption{Независимые от приложения стратегии тестирования. Метрика слева -- уникальные состояния, справа -- уникальные Активности.}
    \label{app_free_table}
\end{table}

\begin{figure}[h!]
\centering
\subfloat[Метрика: уникальные состояния.]{\includegraphics[width=430px]{states_app_free.jpg}} \\
\subfloat[Метрика: уникальные Активности.]{\includegraphics[width=430px]{activities_app_free.jpg}}
\caption{Независимые от приложения стратегии тестирования.}
\label{app_free_picture}
\end{figure}

Можно заметить, что на 6 из 10 приложениях подход основанный на использовании нейронной сети уступает классическому алгоритму Q-learning. В результате анализа было выяснено, что за время, отведенное под тестирование (12 минут), алгоритм не успевает собрать достаточно данных для обучения, и тем самым нейронная сеть не успевает выучить нужные шаблоны тестирования. Однако, на четырех приложениях нейронная сеть оказалась лучше. Это связано с тем, что на этих приложениях визуальная составляющая состояний не сильно изменяется при переходе в разные Активности. Можно заметить, что все четыре приложения имеют малое количество уникальных Активностей, что говорит о низкой вариативности структуры графического интерфейса. Это облегчает обучение нейронной сети.

\subsection{Зависимые от приложения подходы}

Повышения производительности тестирования приложения можно добиться, если перейти от идеи тестирования произвольного приложения к тестированию конкретного приложения. С целью улучшения метрик качества, перейдем к рассмотрению моделей, способных тестировать только то приложение, под которое они обучаются. Идея этих методов в том, что состояния и действия в Q-таблице теперь будут означать конкретные состояния и действия текущего приложения. Такая персонализация позволит обучить модель с учетом тонкостей приложения.

Как было сказано ранее, функция награды полностью определяет цель тестирования. Рассмотрим несколько функций наград, способных увеличить эффективность тестирования приложения.

\subsubsection{Награда: частота нажатий}\label{events_count}

Наиболее простым Q-learning подходом для этих целей является модель~\cite{adamo2018reinforcement}. У этого метода функция награды содержит только частоту нажатия определенного действия. Суть метода в том, что изначально Q-значение за любое действие в любом состоянии равно константе. Однако при очередном выборе действия Q-значение за это действие корректируется: уменьшается пропорционально количеству выполнений этого действия в текущем состоянии. Таким образом, уже протестированные действия текущего состояния будут иметь меньшее Q-значение. Q-значения в этом подходе корректируется согласно формуле~(\ref{eq_qlearning}). Функция награды в этом подходе выглядит следующим образом

\begin{equation}
\label{eq_reward}
{\displaystyle
R(s_t, a_t) = \frac{1}{count(s_t, a_t)} ,
}
\end{equation}
где $count(s_t, a_t)$ -- количество раз, которое действие $a_t$ воспроизводилось в состоянии $s_t$. Такой подход будет повторно посещать состояние только в том случае, если он уже посетил все оставшиеся состояния. Результаты тестирования этой стратегии приведены в Таблице~\ref{app_based_table}.

Исходя из формулы~(\ref{eq_reward}) можно заметить, что такой выбор функции награды и обновления Q-значения -- далекий от интеллектуальности подход. Действительно, функция награды зависит только от количества нажатий на интерактивный элемент текущего состояния, что неизбежно приводит к обычному обходу графа состояний. Единственный момент, который отличает эту систему от стандартного обхода -- различные значения $\gamma$ для разных состояний. В этом и следующих подходах фактор дисконтирования $\gamma$ формируется согласно следующей формуле

$$
\gamma = 0.9 e^{\displaystyle{-0.1 |events(s_{t+1})|}} ,
$$
где $events(s_{t+1})$ -- все возможные взаимодействия в состоянии $s_{t+1}$. Такое определение $\gamma$ обоснованно интуитивным предположением о том, что для состояний с меньшим количеством интерактивных элементов нужно отдавать предпочтение будущим наградам. Однако фактор дисконтирования влияет только на степень учета будущих наград. Для увеличения эффективности необходимо усовершенствовать функцию награды. Будем использовать некоторые эвристические предположения для дальнейшего улучшения метрик.

\begin{table}[h]
    \centering
    \begin{tabular}{ | l | *{8}{>{\centering\arraybackslash}p{37}|}}
        \hline
        Приложения & 
        \multicolumn{2}{>{\centering\arraybackslash}p{74}|}{Обратная частота нажатий} & 
        \multicolumn{2}{>{\centering\arraybackslash}p{74}|}{Кол-во интерактивных элементов} &
        \multicolumn{2}{>{\centering\arraybackslash}p{74}|}{Обратное кол-во инт. элементов} &
        \multicolumn{2}{>{\centering\arraybackslash}p{74}|}{Расстояние между состояниями} \\
        \hline
        ALIEXPRESS &
        \textbf{37} &
        \textbf{11} & 
        34 & 
        10 &
        32 &
        9 &
        30 &
        9 \\
        \hline
        APPLEBEES &
        32 &
        7 & 
        30 & 
        7 &
        \textbf{34} &
        \textbf{8} &
        32 &
        7 \\
        \hline
        BOOKING &
        \textbf{116} &
        \textbf{18} & 
        85 & 
        \textbf{18} &
        85 &
        16 &
        86 &
        16 \\
        \hline
        COLORNOTE &
        142 &
        5 & 
        153 & 
        \textbf{6} &
        \textbf{183} &
        5 &
        164 &
        5 \\
        \hline
        DOMINOS &
        \textbf{35} &
        \textbf{1} & 
        34 & 
        \textbf{1} &
        30 &
        \textbf{1} &
        30 &
        \textbf{1} \\
        \hline
        EBAY &
        \textbf{67} &
        \textbf{11} & 
        45 & 
        10 &
        40 &
        \textbf{11} &
        46 &
        \textbf{11} \\
        \hline
        FACEAPP &
        \textbf{54} &
        \textbf{2} & 
        48 & 
        \textbf{2} &
        48 &
        \textbf{2} &
        39 &
        \textbf{2} \\
        \hline
        NYTIMES &
        \textbf{77} &
        11 & 
        45 & 
        11 &
        49 &
        \textbf{12} &
        74 &
        11 \\
        \hline
        WIKIPEDIA &
        74 &
        9 & 
        77 & 
        12 &
        \textbf{95} &
        \textbf{14} &
        46 &
        10 \\
        \hline
        WSJ &
        96 &
        7 & 
        \textbf{117} & 
        \textbf{8} &
        85 &
        7 &
        82 &
        6 \\
        \hline
    \end{tabular}
    \caption{Зависимые от приложения стратегии тестирования. Метрика слева -- уникальные состояния, справа -- Активности.}
    \label{app_based_table}
\end{table}

\subsubsection{Награда: количество интерактивных элементов}\label{possible_events}

Чтобы алгоритм стремился научиться открывать больше новых состояний, ему нужно давать большую награду за переход в состояния, которые потенциально могут привести к более широкому исследованию. Модифицируем функцию награды таким образом, чтобы алгоритм получал большую награду за переход в состояния с большим числом интерактивных элементов. Это позволит учитывать количество интерактивных элементов не только в долгосрочной перспективе (за счет фактора дисконтирования), но и в мгновенной награде. Функция награды в этом случае будет выглядеть так

$$R(s_t, a_t) = \frac{|events(s_{t+1})|}{count(s_t, a_t)},$$
где $count(s_t, a_t)$ -- количество раз, которое действие $a_t$ воспроизводилось в состоянии $s_t$, $events(s_{t+1})$ -- все возможные взаимодействия в состоянии $s_{t+1}$. Результаты тестирования этой стратегии приведены в Таблице~\ref{app_based_table}.

Можно рассмотреть обратную мотивацию. Посещение состояний с большим количеством интерактивных элементов находится в приоритете для Q-learning алгоритма, так как Q-значение вычисляется с учетом максимума на следующем состоянии. Таким образом, вероятность, что максимум будет больше для состояния с большим количество интерактивных элементов, выше, чем для состояний с малым количеством интерактивных элементов. Следовательно, алгоритм может пропускать такие <<бедные>> состояния. Попробуем исправить эту ситуацию тем, что увеличим награду за посещение состояний с малым количеством интерактивных элементов. Это способствует исследованию граничных состояний перед тем, как заходить в потенциально <<богатые>> состояния. Функция награды выглядит следующим образом 

$$R(s_t, a_t) = \frac{1}{|events(s_{t+1})|count(s_t, a_t)},$$
где $count(s_t, a_t)$ -- количество раз, которое действие $a_t$ воспроизводилось в состоянии $s_t$, $events(s_{t+1})$ -- все возможные взаимодействия в состоянии $s_{t+1}$. Результаты тестирования этой стратегии приведены в Таблице~\ref{app_based_table}.

\subsubsection{Награда: расстояние между состояниями}

Можно рассмотреть еще одну эвристику -- если нажатие на экран приводит к состоянию, близкому к предыдущему (например, открыт выпадающий список), то скорее всего это действие менее ценное, чем то, которое привело бы к кардинально новому состоянию (новой Активности). Состояния приложения можно представлять в виде дерева, в котором интерактивные элементы с их атрибутами находятся в узлах, а вложенные элементы порождают иерархию дерева. Следовательно,  мерой расстояния между состояниями может служить расстояние редактирования графов. Установим это расстояние как функцию награды и проведем измерения эффективности тестирования. Функция награды в этом случае будет выглядеть следующим образом

$$R(s_t, a_t) = \frac{dist(s_t, s_{t+1})}{count(s_t, a_t)},$$
где $count(s_t, a_t)$ -- количество раз, которое действие $a_t$ воспроизводилось в состоянии $s_t$, $dist(s_t, s_{t+1})$ -- расстояние редактирования между графами состояний $s_{t}$ и $s_{t+1}$, которое описывается следующей формулой

$$dist(s_{t},s_{t+1}) = \min_{(e_{1},...,e_{k}) \in \mathcal{P}(s_{t},s_{t+1})} \sum_{i=1}^{k} c(e_{i}) ,$$
где  $\mathcal {P}(s_{t},s_{t+1})$ означает набор преобразования графа $s_{t}$ к графу  $s_{t+1}$, а $c(e)\geqslant 0$ равна стоимости каждой операции преобразования $e$. Стоимость вставки и удаления вершины в этой работе равна единице. Результаты тестирования этой стратегии приведены в Таблице~\ref{app_based_table}.

Для наглядности результаты тестирования всех стратегий, зависящих от приложения, представлены в виде диаграмм размаха на Рис.~\ref{app_based_picture}.


В сравнении результатов стратегий, независимых от приложения (Таблица~\ref{app_free_table}), с результатами последних стратегий (Таблица~\ref{app_based_table}) очевидно превосходство вторых. Несмотря на то, что приложение Wikipedia немного снизило показатели, метрики на остальных приложениях сильно возросли. Как и ожидалось, переход на обучение под конкретное приложение позволил повысить эффективность тестирования.

\begin{figure}[h!]
\centering
\subfloat[Метрика: состояния.]{\includegraphics[width=400px]{states_app_based.jpg}} \\
\subfloat[Метрика: Активности.]{\includegraphics[width=400px]{activities_app_based.jpg}}
\caption{Зависимые от приложения стратегии тестирования.}
\label{app_based_picture}
\end{figure}

Если сравнивать между собой эвристики, которые использовались в функциях наград, то однозначного лидера выделить нельзя. С точки зрения метрики уникальных состояний, хороший результат показали награды за обратную частоту нажатий и обратное количество интерактивных элементов. При анализе результатов на приложении Wall Street Journal лучшей стратегией оказывается та, которая считает более ценные состояния с большим количеством интерактивных элементов. Функция наград с расстоянием между состояниями показала хороший результат на приложениях The New York Times и ColorNote, но все равно не стала лидером. Анализ показал, что близкие на внешний вид состояния могут иметь большое расстояние редактирования между деревьями этих состояний. Это не позволяет в полной мере использовать эту эвристику.

Для дальнейшего улучшения качества тестирования можно прибегнуть к приемам, которые используются во многих приложениях обучения с подкреплением и машинного обучения в целом. Рассмотрим применение некоторых из них в поставленной задаче.

\subsection{Эпсилон жадная стратегия}

Хорошей практикой обучения с подкреплением является добавление некоторой случайности на первых этапах обучения. Это позволяет агенту лучше изучить окружение, до того момента, пока алгоритм не обучится. Такая стратегия называется $\varepsilon$-жадная. Суть ее в том, что перед обучением задается число $\varepsilon$ близкое к единице, его роль -- определять вероятность, с которой агент будет делать случайное действие в текущем состоянии вместо того, чтобы соблюдать стратегию. По ходу обучения это число уменьшается, и через определенное количество времени агент действует только согласно стратегии. Такой подход позволяет на начальных этапах отдать предпочтение случайным действиям, нежели еще необученным алгоритмам. Этот метод позволяет производить более широкое исследование за счет смешивания двух алгоритмов: случайного и основанного на модели. Будем использовать в этом подходе функцию награды, описываемую формулой~(\ref{eq_reward}). Результаты тестирования этой стратегии приведены в Таблице~\ref{advansed_table}.


\subsection{Предобучение модели}\label{pretraining}

В теории машинного обучения принято разделять этапы обучения и использования модели. В поставленной задаче можно также воспользоваться этим правилом. Сначала на длительное время запустить изучение приложения для формирования Q-таблицы, а потом прочитать из файла готовую таблицу и на ней проводить тестирование. Важно понимать, что при формировании таблицы не нужно учитывать количество посещений каждого состояния, иначе на этапе тестирования будет сложно достичь часто посещаемых состояний.

В дополнение к предыдущему приему, можно выполнять тестирование в несколько эпизодов. Одна и та же Q-таблица будет переходить от одного эпизода к другому, а каждый эпизод будет начинать тестирование приложения с самого начала. Таким образом, во время тестирования можно имитировать несколько сессий взаимодействия пользователя, что должно повысить эффективность тестирования. Будем проводить тестирование в пять эпизодов, первые четыре из которых будут этапами обучения Q-таблицы, а последний этапом тестирования.

\begin{table}[h!]
    \centering
    \begin{tabular}{ | l | *{6}{>{\centering\arraybackslash}p{50} |}}
        \hline
        Приложения & 
        \multicolumn{2}{>{\centering\arraybackslash}p{110}|}{Эпсилон жадная стратегия} & 
        \multicolumn{2}{>{\centering\arraybackslash}p{110}|}{Предобученная модель} & 
        \multicolumn{2}{>{\centering\arraybackslash}p{110}|}{Предобученная модель с эпсилон жадной стратегией} \\
        \hline
        ALIEXPRESS &
        30 &
        13 & 
        \textbf{80} &
        16 & 
        63 & 
        \textbf{18} \\
        \hline
        APPLEBEES &
        35 &
        \textbf{9} & 
        \textbf{68} &
        8 & 
        63 & 
        8 \\
        \hline
        BOOKING &
        94 &
        17 & 
        \textbf{107} &
        15 & 
        97 & 
        \textbf{18} \\
        \hline
        COLORNOTE &
        130 &
        5 & 
        189 &
        5 & 
        \textbf{190} & 
        \textbf{6} \\
        \hline
        DOMINOS &
        37 &
        \textbf{1} & 
        \textbf{39} &
        \textbf{1} & 
        36 & 
        \textbf{1} \\
        \hline
        EBAY &
        \textbf{60} &
        \textbf{12} & 
        40 &
        9 & 
        34 & 
        11 \\
        \hline
        FACEAPP &
        63 &
        \textbf{2} & 
        \textbf{79} &
        \textbf{2} & 
        59 & 
        \textbf{2} \\
        \hline
        NYTIMES &
        \textbf{68} &
        \textbf{13} & 
        53 &
        10 & 
        60 & 
        11 \\
        \hline
        WIKIPEDIA &
        101 &
        13 & 
        139 &
        \textbf{14} & 
        \textbf{154} & 
        12 \\
        \hline
        WSJ &
        84 &
        8 & 
        117 &
        8 & 
        \textbf{134} & 
        \textbf{9} \\
        \hline
    \end{tabular}
    \caption{Улучшение стратегий тестирования. Метрика слева -- уникальные состояния, справа -- Активности.}
    \label{advansed_table}
\end{table}

На эпизодах обучения будем использовать функцию награды, равную числу интерактивных элементов в новом состоянии без учета количества посещений состояний. Таким образом удастся сформировать честную Q-таблицу, описывающую только количество интерактивных элементов. На эпизоде тестирования будем использовать обученную Q-таблицу, однако для предотвращения выбора одних и тех же действий теперь функция награды будет обратно пропорциональной количеству взаимодействий~(\ref{eq_reward}). Результаты этого подхода представлены в Таблице~\ref{advansed_table}.

Теперь попробуем добавить в эту стратегию элемент случайности. Объединим стратегию обучения в несколько эпизодов с эпсилон жадной стратегией. Будем использовать описанную ранее $\varepsilon$-жадную стратегию, однако в рамках одного эпизода значение $\varepsilon$ уменьшаться не будет. Уменьшение значения $\varepsilon$ будет происходить после эпизода. Таким образом, первый эпизод тестирования будет почти полностью случайным, а последний -- полностью подчиняться Q-learning стратегии. В итоге последний этап тестирования будет использовать обученную $\varepsilon$-жадной стратегией Q-таблицу. Результаты этого подхода представлены в Таблице~\ref{advansed_table}.


Результаты последних трех подходов либо превосходят все рассмотренные ранее стратегии, либо приближаются к лучшим результатам на них. Таким образом, предобучение модели и добавление в нее элемента случайности значимо повысило эффективность тестирования. Интересно заметить, что в обоих случайных подходах метрика Активностей выше, чем в обычной предобученной модели. Из этого следует, что случайность на первых этапах тестирования положительно сказывается на увеличении метрики Активностей.

\subsection{Анализ результатов}

Перед тем как сравнивать подходы обучения с подкреплением с другими инструментами тестирования мобильных приложений, проведем анализ полученных результатов и выберем подход, который производит наиболее эффективное тестирование.

Для начала стоит отметить, что стратегии, которые способны тестировать произвольное приложение, уступают тем, которые обучаются под единственное приложение. Этого следовало ожидать, так как реализация этих алгоритмов связана с рядом упрощений для совместимости с любым приложением.

Улучшение зависимых от приложения стратегий за счет добавления классических приемов машинного обучения приводит к увеличению метрик качества. Так три последние стратегии показали высокую производительность тестирования и превзошли почти все предыдущие стратегии. Можно заметить, что добавление случайности в алгоритм способствует повышению метрики Активности. Однако смешивание двух стратегий (предобучение и случайность) не позволило взять лучшее от каждого из подходов, и результат в среднем оказался ниже. Исключение составили три приложения: Wall Street Journal, ColorNote и Wikipedia. Однако их превосходство над стратегией с предобучением незначительное.

\begin{table}[h]
    \centering
    \begin{tabular}{ | l | *{8}{>{\centering\arraybackslash}p{37}|}}
        \hline
        Приложения & 
        \multicolumn{2}{>{\centering\arraybackslash}p{74}|}{Обратная частота нажатий} & 
        \multicolumn{2}{>{\centering\arraybackslash}p{74}|}{Обратное кол-во инт. элементов} &
        \multicolumn{2}{>{\centering\arraybackslash}p{74}|}{Предобученная модель} &
        \multicolumn{2}{>{\centering\arraybackslash}p{80}|}{Предобученная модель с эпсилон жадной стратегией} \\
        \hline
        ALIEXPRESS &
        37 &
        11 & 
        34 & 
        10 &
        \textbf{80} &
        16 & 
        63 & 
        \textbf{18} \\
        \hline
        APPLEBEES &
        32 &
        7 & 
        34 &
        \textbf{8} &
        \textbf{68} &
        \textbf{8} & 
        63 & 
        \textbf{8} \\
        \hline
        BOOKING &
        \textbf{116} &
        \textbf{18} &
        85 &
        16 &
        107 &
        15 & 
        97 & 
        \textbf{18} \\
        \hline
        COLORNOTE &
        142 &
        5 & 
        183 &
        5 &
        189 &
        5 & 
        \textbf{190} & 
        \textbf{6} \\
        \hline
        DOMINOS &
        35 &
        \textbf{1} & 
        30 &
        \textbf{1} &
        \textbf{39} &
        \textbf{1} & 
        36 & 
        \textbf{1} \\
        \hline
        EBAY &
        \textbf{67} &
        \textbf{11} & 
        40 &
        \textbf{11} &
        40 &
        9 & 
        34 & 
        \textbf{11} \\
        \hline
        FACEAPP &
        54 &
        \textbf{2} & 
        48 &
        \textbf{2} &
        \textbf{79} &
        \textbf{2} & 
        59 & 
        \textbf{2} \\
        \hline
        NYTIMES &
        \textbf{77} &
        11 & 
        49 &
        \textbf{12} &
        53 &
        10 & 
        60 & 
        11 \\
        \hline
        WIKIPEDIA &
        74 &
        9 & 
        95 &
        \textbf{14} &
        139 &
        \textbf{14} & 
        \textbf{154} & 
        12 \\
        \hline
        WSJ &
        96 &
        7 & 
        85 &
        7 &
        117 &
        8 & 
        \textbf{134} & 
        \textbf{9} \\
        \hline
    \end{tabular}
    
    \caption{Сравнение лучших стратегий тестирования. Метрика слева -- уникальные состояния, справа -- Активности.}
    \label{results_table}
\end{table}

В результате анализа двух метрик качества на десяти приложениях можно выделить четыре лидирующих подхода: подход с наградой за обратную частоту нажатий (пункт \ref{events_count}), подход с наградой за обратное количество интерактивных элементов (пункт \ref{possible_events}), подход с предобучением Q-таблицы (пункт \ref{pretraining}), подход с предобучением и случайностью (пункт \ref{pretraining}). Результаты экспериментов этих стратегий можно увидеть в таблице~\ref{results_table}. Сравнивая их метрики качества, можно заметить, что подход с предобучением Q-таблицы ведет себя наиболее стабильно. Даже если его результаты ниже конкурентов, они все равно довольно близки к лидирующим и высоки в сравнении с другими алгоритмами. Таким образом, лучшим из разработанных в данной работе методов является метод с предобучением таблицы. Именно он будет сравниваться с другим инструментом тестирования в следующем пункте.

\subsection{Сравнение с Humanoid}

В предыдущем пункте удалось выделить лидирующий Q-learning подход. Будем использовать его для сравнения подходов обучения с подкреплением с современным подходом к тестированию, основанном на глубоких нейронных сетях Humanoid~\cite{li2019deep}. Humanoid обучается таким образом, чтобы максимально соответствовать человеческому поведению.


\begin{figure}[h]
\subfloat[Метрика: состояния.]{\includegraphics[width=450px]{states_humanoid.jpg}} \\
\subfloat[Метрика: Активности.]{\includegraphics[width=450px]{activities_humanoid.jpg}}
\caption{Сравнение Q-learnign подхода с Humanoid подходом.}
\label{Humanoid}
\end{figure}

Эксперименты с инструментом Humanoid будем проводить при тех же условиях, что использовались для других экспериментов (5 запусков по 12 минут). Результаты сравнения можно видеть на Рис.~\ref{Humanoid}. Подход обучения с подкреплением с предобучением Q-таблицы превзошел инструмент тестирования Humanoid по метрике уникальных состояний. Если сравнивать два подхода по метрике уникальных Активностей, то ни один из подходов значимо не превосходит другой.