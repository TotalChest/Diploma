\section{Исследование и построение решения задачи}
\label{sec:Chapter3} \index{Chapter3}
\todo[inline]{Здесь надо декомпозировать большую задачу из постановки на подзадачи и продолжать этот процесс, пока подзадачи не станут достаточно простыми, чтобы их можно было бы решить напрямую (например, поставив какой-то эксперимент или доказав теорему) или найти готовое решение.}

Общие сведения об обучении с подкреплением.

В данной работе исследуется метод обучения с подкреплением под названием Q-learning, так как это самый распространенный и хорошо изученный подход обучения с подкреплением. Существующие решения по тестированию мобильных приложений используют Q-learning.

Обучение с подкреплением - это одна их больших ветвей машинного обученияю. В отличие от других ветвей, таких как обучение с учителем или без учителя, его алгоритмы обучаются с использованием вознаграждения и наказания за взаимодействия с окружающей средой. Идея таких подходов основана на концепции поведенческой психологии: методом проб и ошибок достичь максимального результата. В методах обучения с подкрелением основными компонентами являются среда и агент. Агент выступает в качестве объекта, который выполняет неограниченные действия в среде для достижения определенной цели. В ходе выполнения последовательности действий агент получает информации о среде. 

Помимо агента и среды в обучении с подкреплением есть еще три сущности: функция вознаграждения, действия и состояние. Состояние описывает текущую ситацию в окружающей среде. В каждом состоянии у агента есть определенное количество возможных дейсвий. За каждое действие предусмотренна награда, которая оценивает важность того или иного действия в текущем состоянии. Задача агента в том, чтобы в каждом состоянии научиться выбирать такое действие, которое приводит его к максимальной суммарной награде.

Q-обучение - это разновидность безмодельной техники обучения с подкреплением. В нашем случае Q-Learning используется для поиска оптимального выбора действия для данного состояния мобильного прилоджния, где различные стратегии устанавливют правила, по которым агент должен отдавать предпочтение определенному действию. После выбора и воспроизведения действия среда изменяет свое состояние на новое и агент получает награду R за выполнение действия A. Для агента основная цель - научиться действовать оптимальным образом, максимизируя совокупное вознаграждение. Вознаграждение, таким образом, дается за выполнение всей последовательности действий.
% До сюда норм
Q-Learning использует свои Q-значения для решения проблем RL. Для каждой политики Π должна быть должным образом определена функция «действие-значение» или функция качества (Q-функция). Тем не менее, значение Q Π (s t; a t) - это ожидаемое совокупное вознаграждение, которое может быть достигнуто путем выполнения последовательности действий, которая начинается с действия a t из s t; а затем следует политике Π.





Информация о нашем окружении для тестирования

Перед тем, как рассказывать о конкретных алгоритмах и их реализициях нужно понимать как происходит взаимодейстиве со самартфоном. В качестве смарфотна во всех экспериментах будет выступать официальный эмулятор операционой системы Android. Удобным инструментом для программного взяимодейсвие с телефоном является утилита командной строки - Android Debug Bridge (adb). Именно ее использует DroidBot[...] для взаимодействия с с подключенным смартфоном. Все реализованные в работе алгоритмы работают на основе генератора тестов DroidBot. Он удобен тем, что может взаимодействовать с приложениями без модификации системы и самого приложения. Это делает DroidBot совместимым со всеми смартфонами и приложениями. Также он предоставляет возможность подключать собстенные стратегии тестирования, что актуально для нашей работы. Еще одна причина, по которой был выбран этот интрумент - подробный отчет о тестировании, из которого можно получить нужные метрики.

% Надо написать о поддерживаемых взвимодействиях из QBE

% Пайплайн взаимодействия (с рисунком)

Приложения

Чтобы сделать процесс тестирования наиболее честным, возьмем для тестирования приложения различных категорий и степеней сложности. Всего будет использоваться 6 современных приложений:  booking, ebay, nytimes, applebees, faceapp, reader. В их числе есть как простые приложения с небольшим количеством состояний, так и масштабные приложения с платформы Google Play. 




Все рассматриваемые алгоритмы условно разобъем на две группы. Первая из них включает алгоритмы, которые обучаются под одно приложение и способны в последствии тестировать только его. Вторая группа алгоритмов может обобщать и переносить свои методы между от приложения к приложению. Таким образом, ожидается, что алгоритмы тестирования конкретного приложения будут более эффективными при тестировании этого приложения, чем обобщенный вариант из второй группы. Обратно, переносимость алгоритма дает позволяет эффективно тестировать большой набор приложений.

Будем вести изложение от простых методов, до более сложных. Начнем с первой группы подходов, которые способны тестировать только одно прилодение.

Наиболее простым методом обучения с подкремпление является Q-learning подход, у которого функция награды содержит только частоту нажатия определенного действия. Суть метода в том, что изначально Q-значение за любое действие в любом состоянии равно константе. Однако, при очередном выборе действия Q-значение за это действие корретируется: уменьшается обратно пропорционально количеству выполнений этого действия. Таким образом, уже протестированные действия текущего состояния будут иметь меньшее Q-значение. Q-значения в этом подходе корректируется согласно следующей формуле:

$$Q(s, e) = R(s, s', e) + \gamma \max_{e' \in E_s} Q(s', e')$$

где $R(s, s', e)$ - награда за переход из состояния $s$ в сотояние $s'$ по нажатию на действие $e$, $\gamma$ - фактор дисконтирования. Результаты тестирования этого подхода с разными гиперпараметрами можно видеть на графике 1. (все запуски по 10 раз по 15 мин, с нулевой табличкой, для каждого приложения)

% Провести тест с замером времени на тестирование, после которого метрики перестают улучшаться (добавить графики)
Постановка задачи требовала от алгоритма ограниченности по времени. В практических приложениях тестирование должно занмать не более 15 минут. В силу специфики нашего алгоритма, нам может потребоваться меньше времени для достаточно эффективного тестирования. Проведем эксперименты на базовом Q-learning алгоритме для вычисления оптимального времени тестирования. Результаты тестирования представленны на графике 2 . Видно, что после ... минут тестирования на разных приложениях улучшения не значимы. Для всех остальных тестов будем использовать это время.
(запуски на 5 мин, 10 мин, 15 мин - по 5 раз на всех приложениях)
По результатам замеров (Рисунок n) можно видеть, что графики замера времени выпулы вверх. Это означнает, что с течением времени эффективность поиска новых сотояний уменьшается. Для некоторых приложений есть ощутимая разница между 10 и 15 минутами тестирования, но для большинства это несущественный прирост метрики. В качестве компромисса будем проводить все дальшейшие эксперименты с ограницением времени 12 минут.

% Необходимо ли делать много замеров? Проверить разброс на приллжениях
Так как этот и последующие алгоритмы не являются полностью детерминированными, возникает необходимость проводить эксерименты по несколько раз и результат усреднять. Проверим насколько сильный разброс возможен при тестировании различных приложений и сделаем вывод о том, сколько запусков достаточно проводить для чистоты экспериментов. На графике можно видеть разбос состояний приложений при запуске каждого из них по 10 раз. Видно, что результат довольно нестабильный, поэтому для получения численного значения местрики будем проводить по пять запусков каждого эксперимента, отбрасывать минимальное и максимальное значение, а оставшиеся три значения усреднять.

Выводы о events count:

Исходя из формулы можно заметить, что такой выбор функции награды и обновления Q-значения - далекий от интелектуальности подход. Действительно, функция награды зависит только от количества нажатий на интерактивный элемент текущего состояния, что неизбежно приводит к обычному обходу графа состояний. Единтвенный момент, который отличает эту систему от стандартного обхода - различные значения $\gamma$ для разных состояний. Однако, фактор дисконтирования влияет только на степень учета будущих наград. Для увеличения эффективности необходимо усовершенствовать функцию награды.


Чтобы алгоритм стремился научиться открывать больше новых сотояний, ему нужно давать большую награду за переход в состояния, которые потенциально могут привести к более широкому исследованию. Модифицируем функцию награды таким образом, чтобы алгоритм получал большую награду за переход в состояния с большим числом интерактивных элементов. Это позволит учитывать количество интерактивных элементов не только в долгосрочной перспективе(за счет фактора дисконтирования), но и в мгновенной награде. Результаты тесторования этого подхода с разными гиперпараметрами можно видеть на рисунке 2.

Выводы:

% Награда за посещение состояний с малым количеством UI-элементов


% Можно добавить эвристику про s(t)/s(t+1)
% состояние - древовидная структура, узлами которой является UI-элементы с их атрибутами
Можно рассмотреть еще одну эвристику - если нажатие на экран приводит состоянию, близкому к предыдущему (например, открыт выпадающий список), то скорее всего это действие менее ценное, чем то, которое привело к кардинально новому состоянию (новому активити). Так так сотояния приложения можно представлять в виде дерева (потом расписать подробнее), то мерой расстояния может служить расстояния редактирования графов. Установим это расстояние как функция награды и проведем измерения эффективности тестирования. Результаты представнлны на рисунке 3.

Выводы:

Для дальнейшего улучшения качества тестирования можно прибегать к хитростям, которые используюся по многих приложениях обучения с подкруплением. Рассмотрим применение некоторых из них в нашей задаче.

Первое, что стоит отметить, что в нашей задаче не используется такого полнятия как скорость обучения (learling rate). Q-значения обновляются согласно функции награды, не учитывая предыдущее Q-значение. Это может привести к переобучению или в нашем случае плохому исследованию некоторых сотояний. Добавление дополнительного параметра способствует более плавному изменению Q-значений.

Также хорошей практикой обучения с подкреплением является добавление некоторой случайности на первых этапах обучения. Это позволяет агенту лучше изучить окружение, до того момента, пока алгоритм не обучится. Такая стратегия называется $\epsilon$-жадная. Суть ее в том, что перед обучением задается число $\epsilon$ близкое к единице, его роль определять вероятность с которой агент будет делать случайное действие в текущем состоянии вместо того, чтобы соблюдать стратегию. По ходу обучения это число уменьшается и через определенной время агент действует только согласно стратегии. Такой подход позволяет отдать предпочтение случайным действиям, нежели еще необученным алгоритмам. % добавить график убывания epsilon

В теории машинного обучения принято разделять этапы обучения и использования модели (хотя и не всегда). В нашей задаче можно также воспользоваться этим правилом. Сначала на длительное время запустить изучение приложения для формирования Q-таблицы, а потом прочитать из файла готовую таблицу и на ней проводить тестирование. Нужно понимать, что при формировании таблицы не нужно учитывать количество посещений каджого сотояния, иначе на этапе тестирования будет сложно достичь часто посещаемых состояний.

В дополнение к предыдущей хитрости, можно выполнять тестирование в несколько эпизодов. Одна и та же Q-таблица будет переходить от одного эпизода к другому, а каждый эпизод будет начинать тестирование приложения с измененными парамерами. Таким образом, во время тестирования можно имитировать несколько сессий взимодействия пользователя, что должно повыть эффективность тестирования.% Написать про вторую Epsilon стратегию






Пункты, которые надо затронуть:

- Общие сведения об обучении с подкреплением

- Как работает Q-learning

- Информация о нашем окружении для тестирования (+ наши приложения)

- Первая группа:

    - Первая реализация Q-learning (графики с разными вариациями)
    
    - Чем плох такой подход (простой оход обход графа)
    
    - Изменения для улучшения (графики с разными вариациями)

        - Тестирование с заранее построенной табличкой (по количеству состояний)
        
        - Сохранение Q-таблицы
        
        - Эпсилон жадная стратегия
        
        - Эпизоды тестирования
        
        - Награда с количеством интерактивных элементов следующего состояния
        
        - Награда с расстоянием между состояниями
        
        - Попробовать ввести логичную награду (сколько новых интерактивных элементоа появилось (или получше придумать))))))

- Вторая группа:

    - Обобщенные состояния
    
    - DQN (глубокое обучение)

    - Почему глубокое плохо походит(долго обучается)