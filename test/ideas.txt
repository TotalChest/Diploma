- Разные коэффициенты и функции дисконтирования
- Выбирать не лучшее состояние, а лучшее с большей вероятносью.
- Разные функции наград (с учетом количества интерактивных элементов: +1 / |s(t+1)|, +|s(t+1)\s(t)| / |s(t+1)|)
- epsilon-greedy (случайность на первых итерациях исследования)
- Добавить TED в Reward для сравнения следующего и предыдущего состояний
- Разделить этапы построения Q-таблицы и тестирования (сохранять таблицу на диск)
- Использовать предсказание human-like в функции наград
