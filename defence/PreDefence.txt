1. Тема.
Использование обучения с подкреплением  в задаче автоматического тестирования мобильных приложений

2. Актуальность.
Операционная система Android является одной из доминирующих на современном рынке смартфонов. Исследования показывают, что на мобильном устройстве в среднем установлено от 60 до 90 приложений и пользователь тратит на приложения более двух частов в день.
Под тестированием в данной работе понимается конечный этап проверки правильности функционирования приложения при взаимодействии с пользователем (также называют тестирование графического интерфейса).
Автоматизация данного тестирования необходима в связи с тем, что ручное тестирование это дорого и ненадежно.
Большие промышленные компании заинтересованы в разработке таких инструментов и в частности в институте разрабатывается такой продукт.
Также актуальность этого направления подтверждают множество современных работ, которые стемятся решить пробему оптимального инструмента для этой задачи.

3.Постановка задачи.
Исследование методов обучения с подкреплением в задаче тестирования графического интерфейса мобильных приложений
Реализация и внедрение алгоритмов обучения с подкреплением в систему тестирования DroidBot + Humanoid
Сравнение разных стратегий и функций наград алгоритмов обучения с подкреплением на основе следующих метрик:
Количество уникальных состояний
Количество уникальных Активностей
Сравнение лучшего алгоритма обучения с подкреплением с алгоритмом основанным на глубоких нейронных сетях (Humanoid). Humanoid будет рассмтрен как основной алгоритм, результаты которого нужно превзойти. 

4. Схема взаимодействия
Общая схема автоматического тестирования состоит из трех компонентов:
- Устройство, на котором производится тестирование
- Инструмент взаимодествия с устройством
- Алгоритм генерации тестовых примеров
Наша задача состоит в том, чтобы заметить алгоритм генерации тестов на алгоритм обучения с подкреплением

5. Есть несколько работ по внедрению алгоритмов обучения с подкреплением в исследовательское тестирования, но они обладают некоторыми недостатками:
- Нет открытого исходного кода с реализацией;
- Используют медленный UI-automator (в отличии от быстрого DroidBot);
- Упрощенный дизайн элементов RL (представление состояние).
Также некоторые, близкие к нашей задаче, работы используют Q-learning подход для десктоп приложений. 
Для реализации был выбран выделенный на слайде подход потому что он хорошо встраивается в инструмент DroidBot и судя по результатам статьи показывает хороший результат.

6. Обучение с подкреплением
Выбраный нами алгоритм использует популярный алгоритм обучения с подкреплением - Q-learning. Он состоит из двух ключевых элементов:
- Окружение (состояние);
- Агент (алгоритм генерации тестов).
Агент на каждом шаге тестирования получает от Окружения текущее состояние приложения и награду за предыдущее действие. С помощью Q-таблицы выбирается наиболее релевантное действие для текущего состояния и Q-таблица обновляется с учетом полученной наргады. Далее действие выполняется в Окружении и считываются новые состояние и награда.

7. Терминология
Для понимания метрик тестирования нужно владеть некоторой терминилогией:
Состояние - уникальный набор интерактивных элементов с их атрибутами в текущий момент времени
Активность - обобщенное состояние введенное разработчиками Android как некоторый класс языка программирования
Действие - жест, воспроизводимый тестирующей системой в текущем состоянии при взаимодействии с некоторым элементом

8. Приложения
Чтобы процесс тестирования был наиболее честным будем использовать приложения различных категорий. Всего будет использоваться шесть современных приложений, каждое из которых можно найти в Google Play. Мы ограничиваемся небольшим набором, так как процесс тестирования времязатратный.

9. Метрики качества
Четыре классические метрики тестирования графического интерфейса:
- Колличество уникальных состояний
- Количество уникальных Активностей
- Покрытие кода приложения (мы используем подход тестирования "черный ящик", поэтому у нас нет доступа к коду)
- Количество найденных ошибок (непоказательная метрика на готовых приложениях)
На графиках показана зависимость метрики качестка первой реализации Q-learning от продолжительности тестирования. Видно, что более длительное тестирование улучшает метрики. Но так как на нас накладываются ограничения по времени, будем проводить эксперименты по 12 минут

10. Недетерминироанность
Также можно видеть что несколько запусков процесса тестирования на одном и том же приложении могут показывать разное качество. Это связвно с тем, что в алгоритмах присутствует элемент случаноссти, что заставляет повторять эксперименты по несколько раз.

11. Стратегии
В ходе работы были попробованы различные эвристики, способные улучшить производительность тестирования. Были реализацованы следующие Q-learning подходы:
    1. Снижение награды за выполенение одного и того же действия (что способствует выполнению новых действий - это и есть первая реализация)
    2. Увеличение награды пропроционально количеству интерактивных элементов в новом сотоянии (что способствует посещать состояния с потонциально большим охватом) или обратнопропорционально (что способствует изучению граничных состояний)
    3. Увеличение награды пропорционально расстоянию между соседними состояниями (что способствет открытию новых Активностей) (В данном случает состояние рассматривается как дерево интерактивных элементов) 
    4. Добаление случайности в выбор взаимодейстия на первых эпатах тестирования (Вводится дополнительный параметр epsilon от 0 до 1, который отвечает за вероятность выбора случайного действия, со временем параметр epsilon уменьшается и стратегия управляется Q-learning алгоритмом)

12. Сравнение стратегий
В таблице показаны результаты тестировая пяти стратегий с метрикой по количеству состояний. Можно видеть, что результат зависит от сложности и категории приложения. На первых трех приложениях преимущество за epsilon-жадным алгоритмом, тогда как на остальных приложениях лидирует алгоритм с наградой за редкие нажатия.

13. Подходы обучения с подкреплением
В классическом использовании алгоритмов обучения с подкреплением можно позаимствоать идею разделения процессов постоения Q-таблицы и ее использование для тестирования. Для этого разобьем весь процесс на независимые эпизоды до 12 минут. Каждый предыдущий эпизод будет передавать таблицу следующему, последний эпизод считается тестовым. В втором варианте в каждый эпизод добавляется убывающая функция, добавляющая случайность. Можно видеть прирост производительности по сравнению с предыдущими подходами.

14. Независимость от приложения
Были также рассмотрены подходы обучения с подкреплением, которые способны тестировать не конкретное приложение, а произвольное приложение. Эта идея сорпяжена рядом упрощений, например введением абстрактного состояния, которое подходит под все приложения. В связи с этим стоит ожидать ухудшения метрик тестировая, взамен на обучение единственной модели под все приложения. Были рассмотрены 2 таких подхода и по предварительным результатам тестирования, они проигрывают предыдущим стратегиям. Точные метрики пока собрать не успели.

15. Результаты
Результаты всех подходов представленны в таблице. Можно видеть, что предобученные модели превосходят модели с онлайн обучением. Этого следовало ожидать.

16. Сравнение с Humanoid
Мы сравнили наш лучший Q-learning подход (с предобучением) с современным интументом тестирования на основе глубоких нейронных сетях Humanoid. В среднем почти на всех приложениях Q-learning подход оказался лучше.

17. Выводы
18. Дальнейшая работа
