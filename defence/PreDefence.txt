1. Тема.
Использование обучения с подкреплением  в задаче автоматического тестирования мобильных приложений

2. Актуальность.
Операционная система Android является одной из доминирующих на современном рынке смартфонов. Исследования показывают, что на мобильном устройстве в среднем установлено от 60 до 90 приложений и пользователь тратит на приложения более двух частов в день.
Под тестированием в данной работе понимается конечный этап проверки правильности функционирования приложения при взаимодействии с пользователем (также называют тестирование графического интерфейса).
Автоматизация данного тестирования необходима в связи с тем, что ручное тестирование это дорого и ненадежно.
Большие промышленные компании заинтересованы в разработке таких инструментов и в частности в институте разрабатывается такой продукт.
Также актуальность этого направления подтверждают множество современных работ на сопутствующие темы.

3.Постановка задачи.
Исследование методов обучения с подкреплением в задаче тестирования графического интерфейса мобильных приложений
Реализация и внедрение алгоритмов обучения с подкреплением в систему тестирования DroidBot
Сравнение различных подходов тестирования на наборе приложений
Сравнение алгоритмов обучения с подкреплением с алгоритмом основанным на глубоких нейронных сетях Humanoid

4. Схема взаимодействия
Общая схема автоматического тестирования состоит из трех компонентов:
- Устройство, на котором производится тестирование
- Инструмент взаимодествия с устройством
- Алгритм генерации тестовых примеров
Наша задача состоит в том, чтобы заметить алгоритм генерации тестов на алгоритм обучения с подкреплением

5. Обучение с подкреплением
Любой алгоритм обучения с подкреплением состоит из двух ключевых элементов:
- Окружение (состояние);
- Агент (алгоритм генерации тестов).
Агент на каждом шаге тестирования получает от Окружения текущее состояние приложения и награду за предыдущее действие. С помощью Q-таблицы выбирается наиболее релевантное действие для текущего состояния и Q-таблица обновляется с учетом полученной наргады. Далее действие выполняется в Окружении и считывается новые состояние и награда.

6. Приложения
Чтобы процесс тестирования был наиболее честным будем использовать приложения различных категорий. Всего будет использоваться шесть современных приложений, каждое из которых можно найти в Google Play.

7. Метрики качества
Четыре классические метрики тестирования графического интерфейса:
- Колличество уникальных состояний ()
- Количество уникальных Активностей ()
- Покрытие кода приложения
- Количество найденных ошибок
Можно показать, что последние две метрики зависят от первых двух, поэтому для оценки эффективности будем использовать только первые 2 метрики.
На графиках показана зависимость метрики качестка от продолжительности тестирования. По всем кривым можно видеть небольшую выпуклость вверх, которая говорит о снжении эффективности тестирвоания со временем. Так как требования к тестированию накладывают ограниченее по времени, будем проводить тестирование по 12 минут.

8. Недетерминироанность
Также можно видеть что несколько запусков процесса тестирования на одном и том же приложении могут показывать разное качество. Это связвно с тем, что в алгоритмах присутствует элемент случаноссти, что заставляет повторять эксперименты по несколько раз.

9. Стратегии
Начнем эксперименты с того, что будем пробавть различные эвристики, способные улучшить производительность тестирования. Были попробованы следующие реализации Q-learning подхода:
    1. Снижение награды за выполенение одного и того же действия (что способствует выполнению новых действий)
    2. Увеличение награды пропроционально количеству интерактивных элементов в новом сотоянии (что способствует посещать состояния с потонциально большим охватом)
    3. Увеличение награды пропорционально расстоянию между соседними состояниями (что способствет открытию новых Активностей) 
    4. Добаление случайности в выбор взаимодейстия на первых эпатах тестирования

10. Стратегии (результаты)
В таблице показаны результаты тестировая четырех стратегий с метрикой по количеству состояний. Можно видеть, что результат зависит от сложности и категории приложения. На первых трех приложениях преимущество за epsilon-жадным алгоритмом, тогда как на остальных приложениях лидирует алгоритм с наградой за редкие нажатия.

11. Подходы обучения с подкреплением
В классическом использовании алгоритмов обучения с подкреплением можно позаимствоать идею разделения процессов постоения Q-таблицы и ее использование для тестирования. Для этого разобьем весь процесс на независимые эпизоды до 12 минут. Каждый предыдущий эпизод будет передавать таблицу следующему, последний эпизод считается тестовым. В втором варианте в каждый эпизод добавляется убывающая функция, добавляющая случайность. Можно видеть прирост производительности по сравнению с предыдущими подходами.

12. Независимость от приложения
Были также рассмотрены подходы обучения с подкреплением, которые способны тестировать не конкретное приложение, а произвольное приложение. Эта идея сорпяжена рядом упрощений, например введением абстрактного состояния, которое подходит под все приложения. В связи с этим стоит ожидать ухудшения метрик тестировая, взамен на обучение единственной модели под все приложения. Были рассмотрены 2 таких подхода и по предварительным результатам тестирования, они проигрывают предыдущим стратегиям. Точные метрики пока собрать не успели.

13. Результаты
Результаты всех подходов представленны в таблице. Можно видеть, что предобученные модели превосходят модели с онлайн обучением. Этого следовало ожидать.

15. Сравнение с Humanoid
Мы сравнили наш лучший Q-learning подход с современным интументом тестирования на основе глубоких нейронных сетях Humanoid. Почти на всех приложениях наш подход оказался лучше.

16. Выводы
