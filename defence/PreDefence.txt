1. Тема.
Использование обучения с подкреплением  в задаче автоматического тестирования мобильных приложений

2. (Под тестированием в данном контексте понимается конечный этап проверки правильности функционирования приложения при взаимодействии с пользователем (exploratory testing). Формально это динамический обход состояний мобильного приложения с целью покрытия как можно большего их числа)
Имеется некоторый генератор, способный взаимодействовать с устройством. Генератор считывает состояние приложения, обрабатывает его и посылает действие приложению. Цель тестирования найти аварийные завершения работы и зависания при взаимодейсткии пользователя с приложением.

3. Актуальность.
Мы рассматриваем систему Android, так так это одна из самых популярных операционных систем с открытым исходным кодом.
Автоматизация данного тестирования необходима в связи с тем, что ручное динамическое тестирование это дорого и ненадежно.
Большие промышленные компании заинтересованы в продуктах автоматического динамического тестирования своих приложений и в частности в институте разрабатывается такой продукт.

4.Постановка задачи.
(Android Activity - Java класс, содержащий UI-элементы;
Состояния - древовидная структура, узлами которой является UI-элементы с их атрибутами)

5. Обучение с подкрепление
Есть несколько работ по внедрению алгоритмов обучения с подкреплением в исследовательское тестирования, но они обладают некоторыми недостатками:
- Нет открытого исходного кода с реализацией;
- Используют медленный UI-automator (в отличии от быстрого DroidBot);
- Упрощенный дизайн элементов RL (представление состояние).
Также некоторые, близкие к нашей задаче, работы используют Q-learning подход для десктоп приложений. 
Для baseline реализации был выбран выделенный на слайде подход потому что он хорошо встраивается в инструмент DroidBot и судя по рельтатам статьи показывает хороший результат.

6. Наша задача состоит в том, чтобы в схеме тестирования заменить модель обхода графа в DroidBot на модель обучения с подкреплением.

7. Baseline реализация Q-learning.
Любой алгоритм обучения с подкреплением состоит из двух ключевых элементов:
- Окружение (приложение);
- Агент (стратегия взаимодействия).
Наш Q-learging алгоритм на каждом шаге тестирования получает от Окружения текущее состояние приложения и награду за предыдущее действие. С помощью Q-таблицы выбирается наиболее релевантное действие для текущего состояния и Q-таблица обновляется с учетом полученной наргады. Далее действие выполняется в Окружении и считывается новые состояние и награда.
Функция награды в связанной работе очень примитивная и учитывает только количество определенных действий в текущем состоянии без учета какой-либо метаинформации о состоянии. Поэтому после реализации baisline, ставилась задача выбора хорошей функции награды.

8. Среди протестированных из других работ функций наград были три основные и их модификации:
- Функция наград, которая учитывает разницу предыдущего и следующего графа состояния с помощью TED;
- Epsilon жадная стратегия + отношение количества узлов предыдущего и следующего графа состояния;
- Количество интерактивных элементов в следующем состоянии.

9. В таблице показаны численные результаты сравнения различных функций наград на 8-ми приложениях. Цифры описывают медиану количества уникальных состояний приложения. Строки означают различные функции наград. Можно видеть, что результат зависит от сложности и категории приложения. На последних трех приложениях преимущество за epsilon-жадным алгоритмом, тогда как на первых трех приложениях лидирует алгоритм с наградой за посещения состояний с малым количеством UI-элементов.

10. Результаты и Планы.
(Результаты на слайде)
Для большей производительности разделить этапы построения Q-таблицы и тестирования приложения. Тут нужно решить вопрос о переносимости одной Q-таблицы на разные приложения. 
Добавление ранжирующего классификатора в функцию награды для выбора наиболее человекоподобного действия. (RICO)
