1. Тема.
Использование обучения с подкреплением  в задаче автоматического тестирования мобильных приложений


2. Тестирование мобильных приложений
Начнем с того, что определим какой вид тестирования проводится. Под тестированием в данной работе понимается проверка готового приложения на наличие сбоев при взаимодействии пользователя с приложением. Обычно такое тестирование проводится вручную. Специальный человек-тестировщик, путем различных взимодействий, посещает состояния приложения и пытается найти сбои.


3. Автоматическое тестирование
Ручное тестирование - это дорого и ненадежно, поэтому разрабатываются специальные инструменты, способные автоматически проводить данное тестирование. Обычно такие инстументы состоят из 3-х связных компонентов: устройство, на котором запущено приложение, тестовый генератор (в нашем случае DroidBot), который предоставляет интерфейс для взаимдействия с устройством и алгоритм генереции нажатия, который выбирет эффективное действие в текущем состоянии. Наша задача состоит в том, чтобы алгоритм генерации нажатия использовал подход обучения с подкреплением для проведения наиболее эффективного тестирования.


4. Актуальность.
Основным фактамом, подтверждающим актуальность этой работы является то, что ручное тестирование это дорого, долго и ненадежно. А так же множество современных работ, которые стемятся решить пробему оптимального инструмента для этой задачи, обладают некоторыми недостатками: невоспроизводимость тестов, медленный тестовый генератор, другая операционная система, упрощенный дизайн элементов


5.Постановка задачи.
В связи с этим в работе ставятся следующие задачи:
- Исследовать применимость методов обучения с подкреплением в задаче тестирования мобильных приложений через взаимодействие с графическим интерфейсом
- Реализовать и внедрить алгоритмы обучения с подкреплением в систему тестирования DroidBot
- Сравнить разные стратегии и функции наград алгоритмов обучения с подкреплением используя метрику уникальных состояний
- Сравнить лучшую стратегию обучения с подкреплением с современным инструментом Humanoid (алгоритм тестирования на основе глубоких нейронных сетей)


6. Обучение с подкреплением
Принцип работы алгоритмов обучения с подкперлением опишем на примере известного алгоритма Q-learnig. Как и любой другой алгоритм обучения с подкреплением он состоит из двух ключевых элементов:
    - Окружение;
    - Агент.
Задача Агента состоит в том, чтобы научиться действовать в Окружении наиболее эффективным образом. Эффективность описывается функцией награды, которая поощряет или штрафует Агента за каждое его действие.


7. Обучение с подкреплением: Пример
В нашей задаче алгоритм работает следующим образом:
    - Для начала Окружение передает текущее состояние приложения Агенту.
    - У Агента есть обучаемая таблица, на основе которой он может выбрать действие, которое ему следует сделать в этом состоянии.
    - Например, нажатие кнопки "Назад". Он передает это действие обратно в Окружение.
    - Окружение выполняет это действие на устройстве и оправляет Агенту новое состояние, а также награду за действие "Назад". Она отражает насколько эффективно было это действие.
Задача Q-learning-а заполнить таблицу правильными значениями. 


8. Независимые от приложения стратегии
Исследование задачи началось с независимых от приложения подходов, то есть подходов, способных тестировать произвольное приложение после единоразового обучения модели. В качестве базового алгоритма была реализована модель из работы 2018 года. Для совместимости со всеми приложениями в ней вводится ряд упрощений, а именного абстрактные состояния и абстрактные действия. 
Введеные понятия показались примитивными, поэтому был разработан алгоритм, который представляет состояние как изображение, а действие как бинарная маска в точке нажатия на этом изображении. Для преобразования состояния использовалась сверточная нейронная сеть, эмурирующая таблицу предсказания.
Функция награды в обоих алгоритмах описывает количество интерактивных элементов в новом стотоянии, то есть вознаграждает за посещение состояний с потенциально большим количеством уникальных состояний.


9. Зависимые от приложения стратегии
Для увеличения эффективности тестирования дальше разрабатывались зависимые от приложения стратегии, которые могут тестировать одно приложение, под которое они обучаются. В базовой версии алгоритма представление состояния описывается списком интерактивных элементов в текущий момент времени. Действие описывается списком возможных взаимодействий в текущий момент времени.
Награда в базовом алгоритме обратнопропорциональна количеству раз, которое действие event воспроизводилось в состоянии state.


10. Зависимые от приложения стратегии
Также проводились эксперименты с другими эвристиками для функций наград, способные увеличить эффективность тестирования:
    1. Увеличение награды пропроционально количеству интерактивных элементов в новом сотоянии (способствует посещать состояния с потециально большим количеством новых состояний)
    2. Уменьшение награды пропроционально количеству интерактивных элементов в новом сотоянии (способствует изучению граничных состояний)
    3. Увеличение награды пропорционально расстоянию между предыдущим и следующим состояниями


11. Улучшение стратегий
Чтобы улучшить последние стрегии использовалось еще несколько идей из теории обучения с подкреплением и машинного обучения в целом:
    1. Добаление случайности в выбор взаимодейстия на первых эпатах тестирования (Вводится дополнительный параметр epsilon от 0 до 1, который отвечает за вероятность выбора случайного действия, со временем параметр epsilon уменьшается и стратегия управляется Q-learning алгоритмом)
    2. Идея разделения тестирования на эпизоды обучения и тестирования. Первые эпизоды считаются обучающими (формируют Q-таблицу), последний - эпизод тестирования.
    3. Объединение стратегий: На первые эпизоды обученя добавим элемент случайности убывающий на последующих эпизодах.


12. Приложения
Для тестирования стратегий использовалось 10 современных приложения с пратформы GooglePlay. Были выбраны приложения разных категорий и степеней сложности.


13. Результаты
На слайде представлены результаты всех разработанных алгоритмов. Строки - стратегии, столбцы - приложения, значения - уникальные состояния, покрытые во время тестирования
    1. Выделенные 2 стратегии - Независимое от приложения тестирование. Видно что в среднем Сверточная нейронная сеть проигрывает подходу основанному на абстрактных стотояниях. Анализ показал, что это связано с недостатком времени и данных для обучения.
    2. Следующая выделенная группа - Зависимые от приложения подходы. Как и ожидалось, эти подходы превосходят предыдущие алгоритмы. Экристики с количеством интерактивных элементов оказались полезными при тестировании приложений ColorNote и Wall Street Journal. Однако награда за расстояния между состояния не показала ожидаемого результата. Это связано с тем, что введнное расстояние между состояниями плохо отражает схожесть графических интерфейсов.
    3. Последняя группа стратегий - улучшенные стратегии. Они в среднем превзошли предыдущие стратегии. По результатам сравнения, лучшей из реализованных стратегий является стратегия с предобучением таблицы.


14. Сравнение с Humanoid
Мы сравнили наш лучший Q-learning подход с предобучением с современным инструментом тестирования на основе глубоких нейронных сетей Humanoid. Как видно из графика в среднем Q-learning подход оказался лучше.


15. Результаты работы
 - Изучены подходы обучения с подкреплением в задаче тестирования мобильных приложения через взаимодействие с графическим интерфейсом.
 - Реализовано и внедрено несколько стратегий обучения с подкреплением в инструмент DroidBot.
 - Проведен сравнительный анализ разработанных подходов на основе метрики уникальных состояний и выбран лучший из них.
 - Лучший реализованный Q-learning подход с предобучением таблицы превзошел современный инструмент тестирования Humanoid.
