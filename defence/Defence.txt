1. Тема.
Использование обучения с подкреплением  в задаче автоматического тестирования мобильных приложений

2. Тестирование мобильных приложений
Какое тестирование проводится? Под тестированием в данной работе понимается конечный этап проверки корректности функционирования приложения при взаимодействии с пользователем.
Обычно такое тестирование проводится вручную. Специальный человек-тестировщик взаимодействует с графическим интерфейсом приложения и пытается найти ошибки или сбои во время работы с приложением.

3. Автоматическое тестирование
Ручное тестирование - это дорого и ненадежно, поэтому разрабатываются специальные инструменты, способные автоматически проводить данное тестирование. Общая схема взаимодейсвия таких иснтрументов представлена на слайде:
    - Есть 3 связанных компонента: устройство, тестовый генератор, алгоритм тестирования.
    - На устройстве запускается приложения для тестирования
    - Тестовый генератор способен взаимодействовать с устройством: получать от него состояние приложения и воспроизводить нажатия.
    - Алгоритм тестирования выбирает какие действия нужно сделать, чтобы тестирование было наиболее эффективным.

4. Автоматическое тестирование: Пример
Разберем на примере как работает автоматическое тестирование:
    - тестовые генератор считывает текущее состояние приложения
    - обрабатывает его, добавляет метаинформацию и передает алгоритму тестирования
    - Алгоритм тестирования принимает решение нажать кнопку "назад" и передает эту информацию тестовому генератору.
    - тестовый генератор, переводит эту информацию во взаиможействие и нажимает на нужную точку на экране.
Наша задача состоит в том, чтобы алгоритм тестирования использовал подход обучения с подкреплением для проведения наиболее эффективного тестирования.

5. Актуальность.
Актуальность этого направления следует из нескольких фактов:
Операционная система Android является одной из доминирующих на современном рынке смартфонов.
Автоматизация данного тестирования необходима в связи с тем, что ручное тестирование это дорого и ненадежно.
Большие промышленные компании заинтересованы в разработке таких инструментов и в частности в институте разрабатывается такой продукт.
Также актуальность этого направления подтверждают множество современных работ, которые стемятся решить пробему оптимального инструмента для этой задачи.

6.Постановка задачи.
Исследовать применимость методов обучения с подкреплением в задаче тестирования мобильных приложений через взаимодействие с графическим интерфейсом
Реализовать и внедрить алгоритмы обучения с подкреплением в систему тестирования DroidBot + Humanoid (существующий подход)
Сравнить разные стратегии и функции наград алгоритмов обучения с подкреплением используя метрику уникальных состояний
Сравнить лучшую стратегию обучения с подкреплением с инструментом Humanoid(Humanoid). Humanoid будет рассматриваться как основной алгоритм, результаты которого нужно превзойти. 

7. Обучение с подкреплением
Принцип работы алгоритмов обучения с подкперлением опишем на примере известного алгоритма Q-learnig. Как и любой другой алгоритм обучения с подкреплением он состоит из двух ключевых элементов:
    - Окружение;
    - Агент.
Задача Агента состоит в том, чтобы научиться действовать в окружении наиболее эффективным образом. Эффективность описывается функцией награды, которая поощряет или штрафует Агента за каждое его действие. Таким образом функция награды должна хорошо описывать цель, к которой стремится Агент.

8. Обучение с подкреплением: Пример
Для понимания схемы на слайде приведем пример работы этого алгоритма в нашей задаче.
    - Для начала Окружение передает текущее состояние приложения Агенту
    - Агент получает состояние и согласно таблицы, выбирает действие, которое ему следует сделать в этом состоянии
    - Например, нажатие кнопки "Назад". Он передет это действие обратно в Окружение.
    - Окружение выполняет это действие на устройстве и оправляет Агенту новое состояние, а также награду за действие "Назад". Она отражает насколько эффективно было это действие.
Задача Q-learning-а заполнить таблицу правильными значениями. 

8. Независимые от приложения стратегии
Для начала были разработаны стратегии которые способны тестировать произвольное приложение. То есть одна модель, которая подходит под тестирование всех приложений. В качестве базового алгоритма была реализована модель из работы 2018 года. В ней вводятся понятия абстрактного состояния и абстрактного действия, которые подходят под все приложения. 
Введеные понятия показались примитивными, поэтому был разработан алгоритм, который представляет состояние как изображение. А для его преобразования использоваласьб сверточная нейронная сеть.
Функция награды в обоих алгоритмах описывает количество интерактивных элементов в новом стотоянии, то есть вознаграждает за посещение состояний с большим количеством интерактивных элементов.

9. Зависимые от приложения стратегии
Для увеличения эффективности тестирования дальше разрабатывались стратегии, которые могут тестировать одно приложение, под которое они обучаются. Представление состояния в этих подходах зависит от приложения и описывается списком интерактивных элементов в текущий момент времени. Действие описывается списком возможных взаимодействий в текущий момент времени.
Награда в этом подходе обратнопропорциональна количеству данных взаимодествий в текущем состоянии (способствует выполнению новых действий)

11. Зависимые от приложения стратегии
Также проводились эксперименты с другими эвристиками для функций наград, способные увеличить эффективность тестирования:
    1. Увеличение награды пропроционально количеству интерактивных элементов в новом сотоянии (способствует посещать состояния с потонциально большим охватом)
    2. Уменьшение награды пропроционально количеству интерактивных элементов в новом сотоянии (способствует изучению граничных состояний)
    3. Увеличение награды пропорционально расстоянию между соседними состояниями. В данном случает состояние рассматривается как дерево интерактивных элементов (способствует исследованию новых состояний) 

11. Улучшение стратегий
Чтобы улучшить эти стрегии реализовывались еще несколько подходов:
    1. Добаление случайности в выбор взаимодейстия на первых эпатах тестирования (Вводится дополнительный параметр epsilon от 0 до 1, который отвечает за вероятность выбора случайного действия, со временем параметр epsilon уменьшается и стратегия управляется Q-learning алгоритмом)
    2. Идея разделения тестирования на эпизоды. Первые эпизоды считаются обучающими (формируют Q-таблицу), последний - эпизод тестирования. Таким образом удалось разделить этапы обучения и тестирования. 
    3. Объединим эти стратегии: На первые эпизоды обученя добавим элемент случайности убывающий на последующих эпизодах. На этапе тестирования случайность отсутствует. 

12. Результаты
На слайде представлены результаты всех описанных стратегий. Тесты проводились на 6-и современных приложениях с платформы Google Play.
    1. Выделенные 2 стратегии - Независимое от приложения тестирование. Видно что почти на всех приложени Сверточная нейронная сеть проиграла подходу основанному на абстрактных стотояниях. Это связано с недостатком времени для обучения.
    2. Выделенные приложения - Зависимые от приложения тесты. Как и ожидалось, эти подходы превосходят предыдущие алгоритмы. Экристики с количеством интерактивных элементов оказались полезными при тестировании первого и последнего приложений. Однако награда за расстояния между состояния не показала ожидаемого результата. Это связано с тем, что расстояние редактирование между состояниями плохо отражает схожесть графических интерфейсов.
    3. Последняя группа приложений - улучшенные стратегии. Они в среднем превзошли предыдущие стратегии. По результатам сравнения, лучшей из реализованных стратегий является стратегия с предобучением Q-таблицы. 

16. Сравнение с Humanoid
Мы сравнили наш лучший Q-learning подход (с предобучением) с современным инструментом тестирования на основе глубоких нейронных сетей Humanoid. Как видно из графика в среднем на всех приложениях Q-learning подход оказался лучше.

17. Выводы

18. Дальнейшая работа
